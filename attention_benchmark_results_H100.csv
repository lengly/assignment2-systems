class_name,forward_mean_ms,forward_std_ms,forward_min_ms,forward_max_ms,backward_mean_ms,backward_std_ms,backward_min_ms,backward_max_ms,memory_before_backward_mb,total_forward_time_s,total_backward_time_s,d_model,seq_len,batch_size,status,error,input_memory_mb,attention_scores_memory_mb,attention_weights_memory_mb,output_memory_mb,forward_memory_mb,backward_memory_mb,total_memory_mb
CompiledScaledDotProductAttention,0.2000808744924143,0.01295227230002638,0.1876354217529297,0.2262592315673828,14.821315184235573,45.25374993681908,0.3917217254638672,143.6142921447754,44.40625,0.0020008087158203125,0.14821314811706543,64,256,8,success,,1.5,2.0,2.0,0.5,6.0,5.5,11.5
ScaledDotProductAttention,0.2959489938803017,0.12021230941172689,0.23102760314941406,0.6232261657714844,4.014396574348211,8.922980166971684,0.720977783203125,29.383182525634766,76.390625,0.0029594898223876953,0.04014396667480469,64,256,8,success,,1.5,2.0,2.0,0.5,6.0,5.5,11.5
CustomAttentionImplementation,0.1392364501953125,0.026931567845167592,0.12087821960449219,0.20551681518554688,1.1070489417761564,0.3363635914865881,0.6265640258789062,1.9228458404541016,72.3125,0.001392364501953125,0.011070489883422852,64,256,8,success,,1.5,2.0,2.0,0.5,6.0,5.5,11.5
PytorchFlashAttention,0.09608268737792969,0.020504305211943574,0.08273124694824219,0.1499652862548828,1.5027045737951994,2.7424588333815336,0.2913475036621094,9.290695190429688,68.28125,0.0009608268737792969,0.015027046203613281,64,256,8,success,,1.5,2.0,2.0,0.5,6.0,5.5,11.5
CusFlashAttnTri,0.14119148545432836,0.027545069315237924,0.12063980102539062,0.20647048950195312,109.7053736448288,344.71994638442993,0.5276203155517578,1090.7952785491943,67.50390625,0.0014119148254394531,1.0970537662506104,64,256,8,success,,1.5,2.0,2.0,0.5,6.0,5.5,11.5
CompiledScaledDotProductAttention,0.9183168294839561,1.330136088654399,0.4467964172363281,4.695892333984375,3.0068159103393555,1.8469765782356262,2.2115707397460938,8.240938186645508,213.375,0.009183168411254883,0.030068159103393555,64,1024,8,success,,6.0,32.0,32.0,2.0,72.0,70.0,142.0
ScaledDotProductAttention,0.8417367935180664,0.30454088118858635,0.6804466247558594,1.4202594757080078,2.6891708839684725,0.16748379857745022,2.5048255920410156,3.089427947998047,213.3125,0.008417367935180664,0.026891708374023438,64,1024,8,success,,6.0,32.0,32.0,2.0,72.0,70.0,142.0
CustomAttentionImplementation,0.47495364560745656,0.2277263265568763,0.3566741943359375,0.9169578552246094,2.0505189895629883,0.29339833417907357,1.8203258514404297,2.8274059295654297,149.0,0.0047495365142822266,0.020505189895629883,64,1024,8,success,,6.0,32.0,32.0,2.0,72.0,70.0,142.0
PytorchFlashAttention,0.10962485976051539,0.03189858762198128,0.09298324584960938,0.19788742065429688,1.4091252814978361,0.1832774723879993,1.2354850769042969,1.8968582153320312,84.125,0.0010962486267089844,0.014091253280639648,64,1024,8,success,,6.0,32.0,32.0,2.0,72.0,70.0,142.0
CusFlashAttnTri,0.27761459932662547,0.17318123718723655,0.20384788513183594,0.7660388946533203,2.363061998039484,0.31238311203196645,1.8305778503417969,2.9256343841552734,81.015625,0.0027761459350585938,0.023630619049072266,64,1024,8,success,,6.0,32.0,32.0,2.0,72.0,70.0,142.0
CompiledScaledDotProductAttention,3.57491965405643,0.3673492174129933,3.4279823303222656,4.610538482666016,7.016968913376331,0.05774752571596764,6.981134414672852,7.179021835327148,2257.5,0.035749197006225586,0.07016968727111816,64,4096,8,success,,24.0,512.0,512.0,8.0,1056.0,1048.0,2104.0
ScaledDotProductAttention,8.679675869643688,0.17592393851373345,8.577346801757812,9.151935577392578,19.10250261425972,0.08400578371947631,19.0582275390625,19.335031509399414,2257.25,0.08679676055908203,0.1910250186920166,64,4096,8,success,,24.0,512.0,512.0,8.0,1056.0,1048.0,2104.0
CustomAttentionImplementation,5.430030636489391,0.7398325833491981,5.065202713012695,6.859302520751953,7.216906640678644,0.2937436802312732,7.10749626159668,8.052587509155273,1232.0,0.05430030822753906,0.07216906547546387,64,4096,8,success,,24.0,512.0,512.0,8.0,1056.0,1048.0,2104.0
PytorchFlashAttention,0.33838750096037984,0.02302054599567782,0.3192424774169922,0.39887428283691406,1.4652967220172286,0.08796704787528142,1.344442367553711,1.6205310821533203,192.5,0.0033838748931884766,0.01465296745300293,64,4096,8,success,,24.0,512.0,512.0,8.0,1056.0,1048.0,2104.0
CusFlashAttnTri,0.47578811063431203,0.010084100722451694,0.46563148498535156,0.492095947265625,2.60426988825202,0.31982449581846595,2.0761489868164062,3.2482147216796875,180.0625,0.004757881164550781,0.026042699813842773,64,4096,8,success,,24.0,512.0,512.0,8.0,1056.0,1048.0,2104.0
CompiledScaledDotProductAttention,14.331984333693981,0.22078750771470368,14.220714569091797,14.949560165405273,29.45554256439209,0.4349391965661198,28.24878692626953,29.74557876586914,8707.0,0.14331984519958496,0.2945554256439209,64,8192,8,success,,48.0,2048.0,2048.0,16.0,4160.0,4144.0,8304.0
ScaledDotProductAttention,36.70144081115723,0.35072574974037707,36.51738166809082,37.41264343261719,81.93127810955048,0.040075294236885384,81.87174797058105,82.0314884185791,8706.5,0.36701440811157227,0.8193128108978271,64,8192,8,success,,48.0,2048.0,2048.0,16.0,4160.0,4144.0,8304.0
CustomAttentionImplementation,18.032312393188477,0.10675018711481243,17.773866653442383,18.187522888183594,27.639389038085938,1.1892049806192517,27.217626571655273,31.022310256958008,4608.0,0.18032312393188477,0.2763938903808594,64,8192,8,success,,48.0,2048.0,2048.0,16.0,4160.0,4144.0,8304.0
PytorchFlashAttention,1.0291815269738436,0.014121614185569342,1.009225845336914,1.0564327239990234,3.7748098839074373,0.011815051038865931,3.7572383880615234,3.7910938262939453,449.0,0.010291814804077148,0.037748098373413086,64,8192,8,success,,48.0,2048.0,2048.0,16.0,4160.0,4144.0,8304.0
CusFlashAttnTri,1.4744519721716642,0.013999883776705246,1.4660358428955078,1.5132427215576172,5.292558576911688,0.035752578696701676,5.269527435302734,5.3882598876953125,424.125,0.01474452018737793,0.05292558670043945,64,8192,8,success,,48.0,2048.0,2048.0,16.0,4160.0,4144.0,8304.0
CompiledScaledDotProductAttention,53.04994434118271,2.791778650134802,52.108049392700195,60.99081039428711,113.0925863981247,0.1616152876522392,112.8394603729248,113.3725643157959,34374.0,0.5304994583129883,1.1309258937835693,64,16384,8,success,,96.0,8192.0,8192.0,32.0,16512.0,16480.0,32992.0
ScaledDotProductAttention,,,,,,,,,,,,64,16384,8,error,"CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 13.12 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 49.69 GiB is allocated by PyTorch, and 15.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
CustomAttentionImplementation,,,,,,,,,,,,64,16384,8,error,"CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 14.12 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 49.44 GiB is allocated by PyTorch, and 14.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
PytorchFlashAttention,3.7631988525390625,0.05743221481679939,3.7078857421875,3.914356231689453,13.505076989531517,0.11885150888701901,13.38338851928711,13.700723648071289,1346.0,0.037631988525390625,0.13505077362060547,64,16384,8,success,,96.0,8192.0,8192.0,32.0,16512.0,16480.0,32992.0
CusFlashAttnTri,5.515742115676403,0.008489881111017894,5.505800247192383,5.529165267944336,20.52762545645237,0.021001731511205435,20.507335662841797,20.5838680267334,1296.25,0.05515742301940918,0.2052762508392334,64,16384,8,success,,96.0,8192.0,8192.0,32.0,16512.0,16480.0,32992.0
CompiledScaledDotProductAttention,0.3772735653910786,0.12613188300747424,0.29468536376953125,0.7076263427734375,13.235378079116344,36.46557033061981,1.4224052429199219,117.01631546020508,96.5,0.003772735595703125,0.1323537826538086,128,256,8,success,,3.0,2.0,2.0,1.0,8.0,7.0,15.0
ScaledDotProductAttention,0.6537198787555099,0.033976939448621124,0.6151199340820312,0.7154941558837891,2.5769472122192383,0.1381751790177077,2.336263656616211,2.819538116455078,96.46875,0.006537199020385742,0.025769472122192383,128,256,8,success,,3.0,2.0,2.0,1.0,8.0,7.0,15.0
CustomAttentionImplementation,0.38914679316803813,0.07639882824150845,0.34499168395996094,0.5948543548583984,1.7932653427124023,0.1445751840947196,1.5859603881835938,1.9562244415283203,88.3125,0.003891468048095703,0.017932653427124023,128,256,8,success,,3.0,2.0,2.0,1.0,8.0,7.0,15.0
PytorchFlashAttention,0.12044906907249242,0.018315624402021058,0.10848045349121094,0.1652240753173828,2.1066428162157536,2.5553167797625065,1.089334487915039,9.37509536743164,80.3125,0.0012044906616210938,0.02106642723083496,128,256,8,success,,3.0,2.0,2.0,1.0,8.0,7.0,15.0
CusFlashAttnTri,0.14572143845725805,0.025187455321429297,0.13065338134765625,0.21195411682128906,153.87985110282898,481.7880392074585,1.0275840759277344,1525.0723361968994,76.75390625,0.00145721435546875,1.5387985706329346,128,256,8,success,,3.0,2.0,2.0,1.0,8.0,7.0,15.0
CompiledScaledDotProductAttention,1.0801792377606034,0.6186965620145202,0.782012939453125,2.8128623962402344,2.7010678313672543,0.6909907679073513,2.332925796508789,4.660844802856445,389.75,0.01080179214477539,0.027010679244995117,128,1024,8,success,,12.0,32.0,32.0,4.0,80.0,76.0,156.0
ScaledDotProductAttention,1.5120267635211349,0.49032480455935,1.2531280517578125,2.4712085723876953,3.087687538936734,0.04598479063133709,3.008127212524414,3.1588077545166016,389.625,0.015120267868041992,0.030876874923706055,128,1024,8,success,,12.0,32.0,32.0,4.0,80.0,76.0,156.0
CustomAttentionImplementation,0.743651413358748,0.2186931815231219,0.6268024444580078,1.2469291687011719,1.6436815494671464,0.6980047910474241,1.1827945709228516,3.3740997314453125,261.0,0.007436513900756836,0.01643681526184082,128,1024,8,success,,12.0,32.0,32.0,4.0,80.0,76.0,156.0
PytorchFlashAttention,0.22211074247024953,0.11832818563561887,0.16760826110839844,0.5526542663574219,1.2027978664264083,0.4007464158348739,0.820159912109375,2.198457717895508,132.25,0.0022211074829101562,0.012027978897094727,128,1024,8,success,,12.0,32.0,32.0,4.0,80.0,76.0,156.0
CusFlashAttnTri,0.33054352388717234,0.03634077438618988,0.2999305725097656,0.4189014434814453,2.3608445189893246,0.19105832325294614,2.0422935485839844,2.7129650115966797,118.015625,0.0033054351806640625,0.02360844612121582,128,1024,8,success,,12.0,32.0,32.0,4.0,80.0,76.0,156.0
CompiledScaledDotProductAttention,7.436824031174183,0.27845322620123625,7.316112518310547,8.218050003051758,16.231011599302292,0.07409818499581888,16.152143478393555,16.392230987548828,4499.0,0.07436823844909668,0.16231012344360352,128,4096,8,success,,48.0,512.0,512.0,16.0,1088.0,1072.0,2160.0
ScaledDotProductAttention,17.420005053281784,0.28365213074721396,17.227649688720703,18.205642700195312,38.95864635705948,0.06638540799031034,38.91897201538086,39.14284706115723,4498.5,0.17420005798339844,0.3895864486694336,128,4096,8,success,,48.0,512.0,512.0,16.0,1088.0,1072.0,2160.0
CustomAttentionImplementation,10.54692268371582,0.8392936433665454,10.166645050048828,12.81285285949707,14.957142062485218,0.2964831073768437,14.785051345825195,15.78664779663086,2448.0,0.1054692268371582,0.14957141876220703,128,4096,8,success,,48.0,512.0,512.0,16.0,1088.0,1072.0,2160.0
PytorchFlashAttention,0.9505510097369552,0.03339344038977288,0.9243488311767578,1.041412353515625,3.666830016300082,0.041342816984979436,3.569364547729492,3.699779510498047,385.0,0.009505510330200195,0.03666830062866211,128,4096,8,success,,48.0,512.0,512.0,16.0,1088.0,1072.0,2160.0
CusFlashAttnTri,0.638127326965332,0.006006644525768934,0.6275177001953125,0.6468296051025391,3.129076911136508,0.14469304005615413,3.0012130737304688,3.502368927001953,328.0625,0.00638127326965332,0.03129076957702637,128,4096,8,success,,48.0,512.0,512.0,16.0,1088.0,1072.0,2160.0
CompiledScaledDotProductAttention,73.99074733257294,0.7403459749184549,73.50397109985352,75.42562484741211,164.84427452087402,0.22598802752327174,164.6735668182373,165.40074348449707,17285.0,0.7399075031280518,1.6484427452087402,128,8192,8,success,,96.0,2048.0,2048.0,32.0,4224.0,4192.0,8416.0
ScaledDotProductAttention,86.22834831476212,39.185091853141785,73.45700263977051,197.7369785308838,165.16545414924622,0.07637911767233163,165.04383087158203,165.269136428833,17285.0,0.86228346824646,1.6516544818878174,128,8192,8,success,,96.0,2048.0,2048.0,32.0,4224.0,4192.0,8416.0
CustomAttentionImplementation,36.49478033185005,0.07198432285804302,36.40866279602051,36.58127784729004,56.69007450342178,1.521076075732708,56.047677993774414,61.00940704345703,9088.0,0.3649477958679199,0.5669007301330566,128,8192,8,success,,96.0,2048.0,2048.0,32.0,4224.0,4192.0,8416.0
PytorchFlashAttention,3.330182982608676,0.06659454084001482,3.288745880126953,3.5109519958496094,12.921405024826527,0.04943915337207727,12.80069351196289,12.993335723876953,834.0,0.03330183029174805,0.12921404838562012,128,8192,8,success,,96.0,2048.0,2048.0,32.0,4224.0,4192.0,8416.0
CusFlashAttnTri,2.1610737312585115,0.03735170321306214,2.1376609802246094,2.257823944091797,8.315038867294788,0.01840609184000641,8.298158645629883,8.364677429199219,720.125,0.021610736846923828,0.08315038681030273,128,8192,8,success,,96.0,2048.0,2048.0,32.0,4224.0,4192.0,8416.0
CompiledScaledDotProductAttention,,,,,,,,,,,,128,16384,8,error,"CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 11.56 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 66.07 GiB is allocated by PyTorch, and 760.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
ScaledDotProductAttention,,,,,,,,,,,,128,16384,8,error,"CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 11.56 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 66.07 GiB is allocated by PyTorch, and 760.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
CustomAttentionImplementation,,,,,,,,,,,,128,16384,8,error,"CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 11.56 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 66.31 GiB is allocated by PyTorch, and 512.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
PytorchFlashAttention,12.67538033425808,0.152374996105209,12.410640716552734,12.869596481323242,48.91083389520645,1.4297538436949253,48.203229904174805,52.75535583496094,2116.0,0.1267538070678711,0.4891083240509033,128,16384,8,success,,192.0,8192.0,8192.0,64.0,16640.0,16576.0,33216.0
CusFlashAttnTri,8.20844154804945,0.08091773634077981,8.156061172485352,8.406400680541992,32.99994394183159,0.01874171539384406,32.97543525695801,33.03718566894531,1888.25,0.08208441734313965,0.3299994468688965,128,16384,8,success,,192.0,8192.0,8192.0,64.0,16640.0,16576.0,33216.0
CompiledScaledDotProductAttention,0.5105495220050216,0.2672508417163044,0.3457069396972656,1.1551380157470703,2.233815146610141,0.7099115755409002,1.5120506286621094,4.042387008666992,160.625,0.005105495452880859,0.022338151931762695,256,256,8,success,,6.0,2.0,2.0,2.0,12.0,10.0,22.0
ScaledDotProductAttention,0.3593921719584614,0.12368158786557615,0.27298927307128906,0.6470680236816406,2.3218155838549137,0.43016852578148246,1.59454345703125,3.1931400299072266,160.625,0.003593921661376953,0.023218154907226562,256,256,8,success,,6.0,2.0,2.0,2.0,12.0,10.0,22.0
CustomAttentionImplementation,0.20103454880882055,0.04069550413987599,0.17714500427246094,0.3123283386230469,1.699256943538785,0.43576021562330425,1.0631084442138672,2.6051998138427734,144.3125,0.002010345458984375,0.016992568969726562,256,256,8,success,,6.0,2.0,2.0,2.0,12.0,10.0,22.0
PytorchFlashAttention,0.08621215965831652,0.010130343071068637,0.07939338684082031,0.1125335693359375,2.066707704216242,3.2215595711022615,0.6780624389648438,11.219978332519531,128.375,0.00086212158203125,0.020667076110839844,256,256,8,success,,6.0,2.0,2.0,2.0,12.0,10.0,22.0
CusFlashAttnTri,0.19156932830810547,0.13893638970330358,0.1266002655029297,0.5781650543212891,263.7810409069061,829.3733596801758,0.9183883666992188,2624.218702316284,113.25390625,0.0019156932830810547,2.637810468673706,256,256,8,success,,6.0,2.0,2.0,2.0,12.0,10.0,22.0
CompiledScaledDotProductAttention,1.4255046844482422,0.41337410220876336,1.2404918670654297,2.5861263275146484,2.9565810691565275,0.33367733703926206,2.8214454650878906,3.902912139892578,838.5,0.014255046844482422,0.029565811157226562,256,1024,8,success,,24.0,32.0,32.0,8.0,96.0,88.0,184.0
ScaledDotProductAttention,2.422905061393976,0.05960615089861676,2.376556396484375,2.567768096923828,5.192232318222523,0.09747972944751382,5.121469497680664,5.459785461425781,838.25,0.024229049682617188,0.05192232131958008,256,1024,8,success,,24.0,32.0,32.0,8.0,96.0,88.0,184.0
CustomAttentionImplementation,1.2650012504309416,0.15326635912060738,1.1920928955078125,1.676797866821289,2.4345635902136564,0.2837234642356634,2.1195411682128906,3.2057762145996094,581.0,0.012650012969970703,0.02434563636779785,256,1024,8,success,,24.0,32.0,32.0,8.0,96.0,88.0,184.0
PytorchFlashAttention,0.3170013369526714,0.01732187593006529,0.3056526184082031,0.3643035888671875,1.7618894344195724,0.41105711716227233,1.455545425415039,2.895355224609375,324.5,0.003170013427734375,0.017618894577026367,256,1024,8,success,,24.0,32.0,32.0,8.0,96.0,88.0,184.0
CusFlashAttnTri,0.18134116544388235,0.023439024516846985,0.164031982421875,0.2384185791015625,2.3039341904222965,0.2478146634530276,2.0394325256347656,2.7718544006347656,264.015625,0.0018134117126464844,0.02303934097290039,256,1024,8,success,,24.0,32.0,32.0,8.0,96.0,88.0,184.0
CompiledScaledDotProductAttention,41.13025590777397,19.312705844640732,34.84487533569336,96.0845947265625,80.6286558508873,0.09595303708920255,80.57975769042969,80.89971542358398,9365.0,0.4113025665283203,0.8062865734100342,256,4096,8,success,,96.0,512.0,512.0,32.0,1152.0,1120.0,2272.0
ScaledDotProductAttention,35.74030473828316,1.817189040593803,34.82389450073242,40.758609771728516,80.63678443431854,0.07510002615163103,80.58595657348633,80.84511756896973,9365.0,0.357403039932251,0.8063678741455078,256,4096,8,success,,96.0,512.0,512.0,32.0,1152.0,1120.0,2272.0
CustomAttentionImplementation,21.278072148561478,0.09096334542846307,21.10743522644043,21.4540958404541,32.610584050416946,1.6187940491363406,32.05108642578125,37.21499443054199,5264.0,0.21278071403503418,0.32610583305358887,256,4096,8,success,,96.0,512.0,512.0,32.0,1152.0,1120.0,2272.0
PytorchFlashAttention,3.65121359936893,0.06292916077654809,3.5598278045654297,3.7817955017089844,15.783190727233887,0.39379880763590336,15.449285507202148,16.528844833374023,1154.0,0.036512136459350586,0.15783190727233887,256,4096,8,success,,96.0,512.0,512.0,32.0,1152.0,1120.0,2272.0
CusFlashAttnTri,1.1863947147503495,0.160845389473252,1.0538101196289062,1.4548301696777344,5.252051167190075,0.10806187492562458,5.198001861572266,5.556583404541016,912.0625,0.011863946914672852,0.0525205135345459,256,4096,8,success,,96.0,512.0,512.0,32.0,1152.0,1120.0,2272.0
CompiledScaledDotProductAttention,,,,,,,,,,,,256,8192,8,error,"CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 12.30 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 50.38 GiB is allocated by PyTorch, and 15.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
ScaledDotProductAttention,,,,,,,,,,,,256,8192,8,error,"CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 12.30 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 50.38 GiB is allocated by PyTorch, and 15.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
CustomAttentionImplementation,,,,,,,,,,,,256,8192,8,error,"CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 11.55 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 15.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
PytorchFlashAttention,13.528132811188698,0.38419466000050306,13.000249862670898,14.116048812866211,58.37838724255562,1.0770781664177775,57.21116065979004,59.938669204711914,2372.0,0.13528132438659668,0.5837838649749756,256,8192,8,success,,192.0,2048.0,2048.0,64.0,4352.0,4288.0,8640.0
CusFlashAttnTri,3.947925753891468,0.04356864155852236,3.9093494415283203,4.051685333251953,18.96221563220024,0.07092266605468467,18.781423568725586,19.05536651611328,1888.125,0.03947925567626953,0.189622163772583,256,8192,8,success,,192.0,2048.0,2048.0,64.0,4352.0,4288.0,8640.0
CompiledScaledDotProductAttention,,,,,,,,,,,,256,16384,8,error,"CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 10.30 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 68.06 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
ScaledDotProductAttention,,,,,,,,,,,,256,16384,8,error,"CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 11.30 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 63.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
CustomAttentionImplementation,,,,,,,,,,,,256,16384,8,error,"CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 11.30 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 63.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
PytorchFlashAttention,58.815646916627884,4.280234687030315,54.36897277832031,65.55318832397461,226.49744153022766,4.290068056434393,220.16215324401855,235.69035530090332,5192.0,0.5881564617156982,2.264974355697632,256,16384,8,success,,384.0,8192.0,8192.0,128.0,16896.0,16768.0,33664.0
CusFlashAttnTri,15.195918269455433,0.021740745069109835,15.179634094238281,15.253782272338867,74.67248290777206,0.03796365490416065,74.58949089050293,74.72467422485352,4224.25,0.15195918083190918,0.7467248439788818,256,16384,8,success,,384.0,8192.0,8192.0,128.0,16896.0,16768.0,33664.0
CompiledScaledDotProductAttention,1.1012554168701172,1.1854699114337564,0.5593299865722656,4.273176193237305,3.0330896843224764,2.134923357516527,1.6582012176513672,9.007453918457031,384.9375,0.011012554168701172,0.030330896377563477,512,256,8,success,,12.0,2.0,2.0,4.0,20.0,16.0,36.0
ScaledDotProductAttention,0.55780413094908,0.08074405195657164,0.5230903625488281,0.7843971252441406,2.8666972648352385,1.631297403946519,1.708984375,7.335662841796875,384.9375,0.005578041076660156,0.028666973114013672,512,256,8,success,,12.0,2.0,2.0,4.0,20.0,16.0,36.0
CustomAttentionImplementation,0.34325121669098735,0.06277400825638324,0.31876564025878906,0.5204677581787109,1.449537230655551,1.1689085513353348,0.9746551513671875,4.763603210449219,352.3125,0.0034325122833251953,0.014495372772216797,512,256,8,success,,12.0,2.0,2.0,4.0,20.0,16.0,36.0
PytorchFlashAttention,,,,,,,,,,,,512,256,8,error,No available kernel. Aborting execution.,,,,,,,
CusFlashAttnTri,0.2046823501586914,0.06589515396626666,0.156402587890625,0.36787986755371094,3.9475681260228157,6.7077865824103355,1.5006065368652344,23.009777069091797,258.25390625,0.002046823501586914,0.03947567939758301,512,256,8,success,,12.0,2.0,2.0,4.0,20.0,16.0,36.0
CompiledScaledDotProductAttention,2.734613372012973,0.971994420979172,2.390146255493164,5.49626350402832,6.39729481190443,0.03489008668111637,6.354808807373047,6.48045539855957,2120.0,0.027346134185791016,0.06397294998168945,512,1024,8,success,,48.0,32.0,32.0,16.0,128.0,112.0,240.0
ScaledDotProductAttention,5.214452743530273,0.4964691470377147,5.008935928344727,6.615638732910156,12.479066848754883,0.07650748011656106,12.41159439086914,12.679815292358398,2119.5,0.052144527435302734,0.12479066848754883,512,1024,8,success,,48.0,32.0,32.0,16.0,128.0,112.0,240.0
CustomAttentionImplementation,2.7954101096838713,0.512936559971422,2.5930404663085938,4.248857498168945,6.199741270393133,0.5085771554149687,5.865812301635742,7.509708404541016,1605.0,0.0279541015625,0.061997413635253906,512,1024,8,success,,48.0,32.0,32.0,16.0,128.0,112.0,240.0
PytorchFlashAttention,,,,,,,,,,,,512,1024,8,error,No available kernel. Aborting execution.,,,,,,,
CusFlashAttnTri,0.33075810642912984,0.06950437091290951,0.2951622009277344,0.5216598510742188,8.734273724257946,0.14373341400641948,8.643865585327148,9.13381576538086,844.015625,0.0033075809478759766,0.08734273910522461,512,1024,8,success,,48.0,32.0,32.0,16.0,128.0,112.0,240.0
CompiledScaledDotProductAttention,79.09543812274933,14.457414858043194,74.0518569946289,120.2082633972168,180.81466853618622,1.2176053132861853,179.59833145141602,182.92474746704102,20634.0,0.7909543514251709,1.8081467151641846,512,4096,8,success,,192.0,512.0,512.0,64.0,1280.0,1216.0,2496.0
ScaledDotProductAttention,87.00735867023468,39.25120458006859,74.01323318481445,198.70567321777344,180.69536983966827,1.1277336161583662,179.793119430542,183.59375,20634.0,0.8700735569000244,1.8069536685943604,512,4096,8,success,,192.0,512.0,512.0,64.0,1280.0,1216.0,2496.0
CustomAttentionImplementation,47.095440328121185,0.3858990385197103,46.60367965698242,47.97863960266113,83.08970928192139,0.831918849144131,81.46834373474121,84.25283432006836,12432.0,0.47095441818237305,0.8308970928192139,512,4096,8,success,,192.0,512.0,512.0,64.0,1280.0,1216.0,2496.0
PytorchFlashAttention,,,,,,,,,,,,512,4096,8,error,No available kernel. Aborting execution.,,,,,,,
CusFlashAttnTri,3.160500433295965,0.1353244442725554,3.0357837677001953,3.4656524658203125,115.82658439874649,1.0856658918783069,115.13900756835938,118.27802658081055,3232.0625,0.03160500526428223,1.1582658290863037,512,4096,8,success,,192.0,512.0,512.0,64.0,1280.0,1216.0,2496.0
CompiledScaledDotProductAttention,,,,,,,,,,,,512,8192,8,error,"CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 6.44 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 70.39 GiB is allocated by PyTorch, and 176.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
ScaledDotProductAttention,,,,,,,,,,,,512,8192,8,error,"CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 6.44 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 70.39 GiB is allocated by PyTorch, and 176.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
CustomAttentionImplementation,,,,,,,,,,,,512,8192,8,error,"CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 4.44 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 72.38 GiB is allocated by PyTorch, and 192.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
PytorchFlashAttention,,,,,,,,,,,,512,8192,8,error,No available kernel. Aborting execution.,,,,,,,
CusFlashAttnTri,11.422967538237572,0.18695369362831116,11.333703994750977,11.930704116821289,467.8587019443512,35.044848918914795,456.5894603729248,567.5973892211914,6528.125,0.11422967910766602,4.678586959838867,512,8192,8,success,,384.0,2048.0,2048.0,128.0,4608.0,4480.0,9088.0
CompiledScaledDotProductAttention,,,,,,,,,,,,512,16384,8,error,"CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 63.94 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 13.06 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
ScaledDotProductAttention,,,,,,,,,,,,512,16384,8,error,"CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 63.94 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 13.06 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
CustomAttentionImplementation,,,,,,,,,,,,512,16384,8,error,"CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 63.94 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 13.06 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",,,,,,,
PytorchFlashAttention,,,,,,,,,,,,512,16384,8,error,No available kernel. Aborting execution.,,,,,,,
CusFlashAttnTri,43.74482482671738,0.005894347395951627,43.73455047607422,43.752193450927734,1818.1064128875732,0.9445042232982814,1816.5099620819092,1819.7178840637207,13504.25,0.43744826316833496,18.18106460571289,512,16384,8,success,,768.0,8192.0,8192.0,256.0,17408.0,17152.0,34560.0
